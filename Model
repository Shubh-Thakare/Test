import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris

# Load the Iris dataset
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = iris.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Function for forward selection
def forward_selection(X, y, model):
    selected_features = []
    while len(selected_features) < X.shape[1]:
        remaining_features = list(set(X.columns) - set(selected_features))
        best_score = 0
        best_feature = None
        for feature in remaining_features:
            current_features = selected_features + [feature]
            X_subset = X[current_features]
            
            # Train the model and evaluate its performance
            model.fit(X_subset, y)
            y_pred = model.predict(X_subset)
            score = accuracy_score(y, y_pred)
            
            # Update the best feature if the current score is higher
            if score > best_score:
                best_score = score
                best_feature = feature
        
        # Add the best feature to the selected features
        selected_features.append(best_feature)
        print(f"Selected Features: {selected_features}, Accuracy: {best_score:.4f}")
    
    return selected_features

# Create a logistic regression model
logreg = LogisticRegression(solver='liblinear')

# Perform forward selection
selected_features = forward_selection(X_train, y_train, logreg)






import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import statsmodels.api as sm

# Generate a sample dataset (replace this with your actual dataset)
np.random.seed(42)
X = pd.DataFrame(np.random.rand(100, 5), columns=['feature1', 'feature2', 'feature3', 'feature4', 'feature5'])
y = np.random.randint(0, 2, size=100)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Function to perform stepwise variable selection
def stepwise_selection(X, y, initial_list=[], threshold_in=0.05, threshold_out=0.05, verbose=True):
    included = list(initial_list)
    while True:
        changed = False
        # Forward step
        excluded = list(set(X.columns) - set(included))
        new_pval = pd.Series(index=excluded, dtype=float)
        for new_column in excluded:
            model = sm.Logit(y, sm.add_constant(pd.DataFrame(X[included + [new_column]]))).fit(disp=0)
            new_pval[new_column] = model.pvalues[new_column]
        best_feature = new_pval.idxmin()
        if new_pval.min() < threshold_in:
            included.append(best_feature)
            changed = True
            if verbose:
                print(f'Add  {best_feature} with p-value {new_pval.min()}')
        
        # Backward step
        model = sm.Logit(y, sm.add_constant(pd.DataFrame(X[included]))).fit(disp=0)
        pvalues = model.pvalues.drop('const')
        worst_feature = pvalues.idxmax()
        if pvalues.max() > threshold_out:
            included.remove(worst_feature)
            changed = True
            if verbose:
                print(f'Drop {worst_feature} with p-value {pvalues.max()}')
        
        if not changed:
            break
    
    return included

# Perform stepwise variable selection
selected_features = stepwise_selection(X_train, y_train)

# Fit the final model using selected features
final_model = LogisticRegression()
final_model.fit(X_train[selected_features], y_train)

# Evaluate the model
accuracy = final_model.score(X_test[selected_features], y_test)
print(f'Model accuracy on test set: {accuracy}')







import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import statsmodels.api as sm

# Load your dataset
# Assuming 'X' contains the features and 'y' contains the target variable
# Modify this part based on your dataset
# For example:
# X = your_data.drop('target_variable', axis=1)
# y = your_data['target_variable']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Forward selection using logistic regression
def forward_selection(X, y, initial_list=[], threshold_in=0.01, verbose=True):
    included = list(initial_list)
    while True:
        changed = False
        excluded = list(set(X.columns) - set(included))
        new_pval = pd.Series(index=excluded, dtype=float)
        for new_column in excluded:
            model = sm.Logit(y, sm.add_constant(X[included + [new_column]])).fit(disp=0)
            new_pval[new_column] = model.pvalues[new_column]
        best_pval = new_pval.min()
        if best_pval < threshold_in:
            best_feature = new_pval.idxmin()
            included.append(best_feature)
            changed = True
            if verbose:
                print(f'Add  {best_feature} with p-value {best_pval}')
        if not changed:
            break
    return included

# Perform forward selection
selected_features = forward_selection(X_train, y_train)

# Train the final model using the selected features
final_model = LogisticRegression()
final_model.fit(X_train[selected_features], y_train)

# Make predictions on the test set
y_pred = final_model.predict(X_test[selected_features])

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')







import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE
from sklearn.metrics import accuracy_score

# Load the Iris dataset as an example
iris = load_iris()
X = iris.data
y = iris.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a random forest classifier (you can replace this with any other classifier)
clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Initialize RFE
rfe = RFE(clf, n_features_to_select=3)

# Fit RFE
rfe.fit(X_train, y_train)

# Get the selected features
selected_features = np.where(rfe.support_)[0]

# Use the selected features to train the model
clf.fit(X_train[:, selected_features], y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test[:, selected_features])

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Selected Features:", selected_features)
print("Accuracy:", accuracy)

